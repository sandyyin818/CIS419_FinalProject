{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
      "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
      "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
      "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
      "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n",
      "5             0        0       0       0              0  \n",
      "6             1        1       0       1              0  \n",
      "7             0        0       0       0              0  \n",
      "8             0        0       0       0              0  \n",
      "9             0        0       0       0              0  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# #check file paths for data paths \n",
    "# from os import listdir\n",
    "\n",
    "# #list files/directories in current directory\n",
    "# for file in listdir(\"./\"):\n",
    "#     print(file)\n",
    "    \n",
    "train_path = \"./data/train.csv\"\n",
    "test_path = \"./data/test.csv\"\n",
    "test_labels_path = \"./data/test_labels.csv\"\n",
    "\n",
    "#load data with pandas and return dataframes \n",
    "def load_data(train_path, test_path, test_labels_path):\n",
    "    train = pd.read_csv(train_path)\n",
    "    print(train[0:10])\n",
    "    test = pd.read_csv(test_path)\n",
    "    test_labels = pd.read_csv(test_labels_path)\n",
    "    cols_target = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']\n",
    "    return train, test, test_labels, cols_target\n",
    "\n",
    "\n",
    "train, test, test_labels, cols = load_data(train_path, test_path, test_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #summarize data\n",
    "# def summary():\n",
    "#     labels = train.iloc[:,2:]\n",
    "#     zero = np.where(np.sum(labels,axis=1)==0)\n",
    "#     # Find the unlabelled percentage\n",
    "#     unlabelled = train[(train['toxic']!=1) & (train['severe_toxic']!=1) &\\\n",
    "#                                  (train['obscene']!=1) & (train['threat']!=1) &\\\n",
    "#                                  (train['insult']!=1) & (train['identity_hate']!=1)]\n",
    "#     print(\"Train data length: \", len(train))\n",
    "#     print(\"Test data length: \", len(test))\n",
    "#     print('\\nPercentage of unlabelled: ', len(unlabelled)/len(train)*100)\n",
    "#     print(train[cols].sum())\n",
    "\n",
    "#     train[cols].sum().plot.bar(title =\"Number of Comments by Label\")\n",
    "\n",
    "# summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(num_to_use): \n",
    "    #temporarily only use 500 \n",
    "    train_comments = train.iloc[0:num_to_use,1]\n",
    "    test_comments = test.iloc[0:num_to_use,1]\n",
    "    \n",
    "    return test_comments, train_comments\n",
    "\n",
    "test_comments, train_comments= get_data(len(train))\n",
    "\n",
    "\n",
    "# each label as a single combined string, e.g. toxic & severe_toxic = 11\n",
    "def get_labels_as_vec(num_to_use):\n",
    "    #labels as numpy \n",
    "    labels = train.iloc[0:num_to_use,2:]\n",
    "    labels['vector_label'] = labels.astype(str).values.sum(axis=1)\n",
    "    train_labels = labels['vector_label']\n",
    "    return train_labels\n",
    "\n",
    "#alternate : each label as a separate pd column\n",
    "def get_labels_as_cols(num_to_use):\n",
    "    #labels as pd dataframes\n",
    "    toxic_train_labels = train.iloc[0:num_to_use,2:3]\n",
    "    severe_toxic_labels = train.iloc[0:num_to_use,3:4] \n",
    "    obscene_labels = train.iloc[0:num_to_use,4:5] \n",
    "    threat_labels = train.iloc[0:num_to_use,5:6] \n",
    "    insult_labels = train.iloc[0:num_to_use,6:7] \n",
    "    identity_hate_labels = train.iloc[0:num_to_use,7:8] \n",
    "    \n",
    "    return toxic_train_labels, severe_toxic_labels, obscene_labels, threat_labels, insult_labels, identity_hate_labels\n",
    "\n",
    "toxic_train_labels, severe_toxic_labels, obscene_labels, threat_labels, insult_labels, identity_hate_labels = get_labels_as_cols(len(train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 2845813)\n"
     ]
    }
   ],
   "source": [
    "#vectorize inputs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def vectorize_tfidf(): \n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, stop_words='english')\n",
    "    features_train = vectorizer.fit_transform(train_comments)\n",
    "    features_test = vectorizer.fit_transform(test_comments)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "#     print(train_vocab)\n",
    "    return features_train, features_test, vectorizer\n",
    "\n",
    "def vectorize_count():\n",
    "    vectorizer = CountVectorizer(max_df=0.5, stop_words='english', ngram_range=(1,2))\n",
    "    features_train = vectorizer.fit_transform(train_comments)\n",
    "    features_test = vectorizer.fit_transform(test_comments)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "#     print(train_vocab)\n",
    "    return features_train, features_test, vectorizer\n",
    "\n",
    "\n",
    "count_features_train, count_features_test, count_vectorizer = vectorize_count()\n",
    "\n",
    "tfidf_features_train, tfidf_features_test, tfidf_vectorizer = vectorize_tfidf()\n",
    "\n",
    "print(count_features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import make_scorer, recall_score, precision_score, accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def train_model(labels):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(features_train, labels)\n",
    "    score = model.score(features_train, labels)\n",
    "    return score, model\n",
    "\n",
    "\n",
    "def get_avg_cvd_score(model, X, y):\n",
    "    to_get = { 'accuracy' : make_scorer(accuracy_score),\n",
    "    'precision' : make_scorer(precision_score, average='macro'),\n",
    "    'recall' : make_scorer(recall_score, average='macro')}\n",
    "    results = cross_validate(model, X, y, scoring=to_get, cv=5)\n",
    "\n",
    "    return np.average(results['test_accuracy']), np.average(results['test_precision']), np.average(results['test_recall'])\n",
    "\n",
    "#toxic\n",
    "score_toxic, _ = train_model(toxic_train_labels.values.ravel())\n",
    "toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, toxic_train_labels.values.ravel())\n",
    "print(\"toxic\")\n",
    "print(\"training accuracy: \" + str(score_toxic))\n",
    "print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "print(\"precision: \" +str(toxic_precision))\n",
    "print(\"recall: \" +str(toxic_recall))\n",
    "print(\"\\n\")\n",
    "\n",
    "#severe_toxic_labels\n",
    "print(\"severe_toxic\")\n",
    "score_severe_toxic, _ = train_model(severe_toxic_labels.values.ravel())\n",
    "toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, severe_toxic_labels.values.ravel())\n",
    "print(score_severe_toxic)\n",
    "print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "print(\"precision: \" +str(toxic_precision))\n",
    "print(\"recall: \" +str(toxic_recall))\n",
    "print(\"\\n\")\n",
    "\n",
    "#obscene_labels\n",
    "print(\"obscene\")\n",
    "score_obscene, _ = train_model(obscene_labels.values.ravel())\n",
    "toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, obscene_labels.values.ravel())\n",
    "print(score_obscene)\n",
    "print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "print(\"precision: \" +str(toxic_precision))\n",
    "print(\"recall: \" +str(toxic_recall))\n",
    "print(\"\\n\")\n",
    "\n",
    "#threat_labels\n",
    "print(\"threat\")\n",
    "score_threat, _ = train_model(threat_labels.values.ravel())\n",
    "toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, threat_labels.values.ravel())\n",
    "print(score_threat)\n",
    "print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "print(\"precision: \" +str(toxic_precision))\n",
    "print(\"recall: \" +str(toxic_recall))\n",
    "print(\"\\n\")\n",
    "\n",
    "#insult_labels\n",
    "print(\"insult\")\n",
    "score_insult, _ = train_model(insult_labels.values.ravel())\n",
    "toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, insult_labels.values.ravel())\n",
    "print(score_insult)\n",
    "print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "print(\"precision: \" +str(toxic_precision))\n",
    "print(\"recall: \" +str(toxic_recall))\n",
    "print(\"\\n\")\n",
    "\n",
    "#identity_hate_labels\n",
    "print(\"identity_hate\")\n",
    "score_identity_hate, _ = train_model(identity_hate_labels.values.ravel())\n",
    "toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, identity_hate_labels.values.ravel())\n",
    "print(score_identity_hate)\n",
    "print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "print(\"precision: \" +str(toxic_precision))\n",
    "print(\"recall: \" +str(toxic_recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "before: 63978\n",
      "before: 63978\n",
      "after: 63978\n",
      "after: 63978\n",
      "labels:                   id  toxic  severe_toxic  obscene  threat  insult  \\\n",
      "5   0001ea8717f6de06      0             0        0       0       0   \n",
      "7   000247e83dcc1211      0             0        0       0       0   \n",
      "11  0002f87b16116a7f      0             0        0       0       0   \n",
      "13  0003e1cccfd5a40a      0             0        0       0       0   \n",
      "14  00059ace3e3e9a53      0             0        0       0       0   \n",
      "16  000663aff0fffc80      0             0        0       0       0   \n",
      "17  000689dd34e20979      0             0        0       0       0   \n",
      "19  000844b52dee5f3f      0             0        0       0       0   \n",
      "21  00091c35fa9d0465      1             0        0       0       0   \n",
      "22  000968ce11f5ee34      0             0        0       0       0   \n",
      "\n",
      "    identity_hate  \n",
      "5               0  \n",
      "7               0  \n",
      "11              0  \n",
      "13              0  \n",
      "14              0  \n",
      "16              0  \n",
      "17              0  \n",
      "19              0  \n",
      "21              0  \n",
      "22              0  \n"
     ]
    }
   ],
   "source": [
    "#clean test data\n",
    "# print(test[0:5])\n",
    "# print(test_labels.loc[test_labels['toxic'] == 1])\n",
    "# print(test_labels[0:15])\n",
    "\n",
    "#remove all unlabeled test data \n",
    "unlabeled = []\n",
    "unlabeled.extend(test_labels.index[test_labels['toxic'] == -1])\n",
    "print(len(test_labels.loc[test_labels['toxic'] == -1]))\n",
    "print(\"before: \" + str(len(test)))\n",
    "print(\"before: \" + str(len(test_labels)))\n",
    "test_labels = test_labels.drop(test_labels.index[unlabeled])\n",
    "test = test.drop(test.index[unlabeled])\n",
    "print(\"after: \" + str(len(test)))\n",
    "print(\"after: \" + str(len(test_labels)))\n",
    "print(\"labels: \" + str(test_labels[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "[0 0 0 ... 0 1 0]\n",
      "severe_toxic\n",
      "obscene\n",
      "threat\n",
      "insult\n",
      "identity_hate\n",
      "<bound method NDFrame.head of        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
      "0          0             0        0       0       0              0\n",
      "1          0             0        0       0       0              0\n",
      "2          0             0        0       0       0              0\n",
      "3          0             0        0       0       0              0\n",
      "4          0             0        0       0       0              0\n",
      "5          0             0        0       0       0              0\n",
      "6          0             0        0       0       0              0\n",
      "7          0             0        0       0       0              0\n",
      "8          0             0        0       0       0              0\n",
      "9          0             0        0       0       0              0\n",
      "10         0             0        0       0       0              0\n",
      "11         0             0        0       0       0              0\n",
      "12         0             0        0       0       0              0\n",
      "13         0             0        0       0       0              0\n",
      "14         0             0        0       0       0              0\n",
      "15         0             0        0       0       0              0\n",
      "16         0             0        0       0       0              0\n",
      "17         0             0        0       0       0              0\n",
      "18         0             0        0       0       0              0\n",
      "19         0             0        0       0       0              0\n",
      "20         0             0        0       0       0              0\n",
      "21         0             0        0       0       0              0\n",
      "22         0             0        0       0       0              0\n",
      "23         0             0        0       0       0              0\n",
      "24         1             0        1       0       1              0\n",
      "25         0             0        0       0       0              0\n",
      "26         0             0        0       0       0              0\n",
      "27         0             0        0       0       0              0\n",
      "28         0             0        0       0       0              0\n",
      "29         1             0        1       0       1              0\n",
      "...      ...           ...      ...     ...     ...            ...\n",
      "63948      0             0        0       0       0              0\n",
      "63949      0             0        0       0       0              0\n",
      "63950      0             0        0       0       0              0\n",
      "63951      0             0        0       0       0              0\n",
      "63952      0             0        0       0       0              0\n",
      "63953      0             0        0       0       0              0\n",
      "63954      0             0        0       0       0              0\n",
      "63955      0             0        0       0       0              0\n",
      "63956      0             0        0       0       0              0\n",
      "63957      0             0        0       0       0              0\n",
      "63958      0             0        0       0       0              0\n",
      "63959      0             0        0       0       0              0\n",
      "63960      0             0        0       0       0              0\n",
      "63961      0             0        0       0       0              0\n",
      "63962      0             0        0       0       0              0\n",
      "63963      0             0        0       0       0              0\n",
      "63964      0             0        0       0       0              0\n",
      "63965      0             0        0       0       0              0\n",
      "63966      0             0        0       0       0              0\n",
      "63967      0             0        0       0       0              0\n",
      "63968      0             0        0       0       0              0\n",
      "63969      0             0        0       0       0              0\n",
      "63970      0             0        0       0       0              0\n",
      "63971      0             0        0       0       0              0\n",
      "63972      0             0        0       0       0              0\n",
      "63973      0             0        0       0       0              0\n",
      "63974      0             0        0       0       0              0\n",
      "63975      0             0        0       0       0              0\n",
      "63976      1             0        1       0       0              0\n",
      "63977      0             0        0       0       0              0\n",
      "\n",
      "[63978 rows x 6 columns]>\n",
      "63978\n",
      "labels:                   id  toxic  severe_toxic  obscene  threat  insult  \\\n",
      "5   0001ea8717f6de06      0             0        0       0       0   \n",
      "7   000247e83dcc1211      0             0        0       0       0   \n",
      "11  0002f87b16116a7f      0             0        0       0       0   \n",
      "13  0003e1cccfd5a40a      0             0        0       0       0   \n",
      "14  00059ace3e3e9a53      0             0        0       0       0   \n",
      "16  000663aff0fffc80      0             0        0       0       0   \n",
      "17  000689dd34e20979      0             0        0       0       0   \n",
      "19  000844b52dee5f3f      0             0        0       0       0   \n",
      "21  00091c35fa9d0465      1             0        0       0       0   \n",
      "22  000968ce11f5ee34      0             0        0       0       0   \n",
      "\n",
      "    identity_hate  \n",
      "5               0  \n",
      "7               0  \n",
      "11              0  \n",
      "13              0  \n",
      "14              0  \n",
      "16              0  \n",
      "17              0  \n",
      "19              0  \n",
      "21              0  \n",
      "22              0  \n"
     ]
    }
   ],
   "source": [
    "#run test data: we will use tfidf for severe toxic, threat, and identity hate\n",
    "#and we will use countvectorizer: less severe labels, i.e. toxic, obscene, and insult.\n",
    "def get_data(num_to_use, data): \n",
    "    comments = data.iloc[0:num_to_use,1]\n",
    "    return comments\n",
    "\n",
    "test_comments = get_data(len(test), test)\n",
    "# print(test_comments)\n",
    "train_comments = get_data(len(train), train)\n",
    "# print(train_comments)\n",
    "\n",
    "def vectorize_tfidf(): \n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, stop_words='english')\n",
    "    features_train = vectorizer.fit_transform(train_comments)\n",
    "    features_test = vectorizer.transform(test_comments)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "#     print(train_vocab)\n",
    "    return features_train, features_test, vectorizer\n",
    "\n",
    "def vectorize_count():\n",
    "    vectorizer = CountVectorizer(max_df=0.5, stop_words='english', ngram_range=(1,2))\n",
    "    features_train = vectorizer.fit_transform(train_comments)\n",
    "    features_test = vectorizer.transform(test_comments)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "#     print(train_vocab)\n",
    "    return features_train, features_test, vectorizer\n",
    "\n",
    "def get_test_labels_as_cols(num_to_use, test_labels):\n",
    "    #labels as pd dataframes\n",
    "    toxic_train_labels = test_labels.iloc[0:num_to_use,2:3]\n",
    "    severe_toxic_labels = test_labels.iloc[0:num_to_use,3:4] \n",
    "    obscene_labels = test_labels.iloc[0:num_to_use,4:5] \n",
    "    threat_labels = test_labels.iloc[0:num_to_use,5:6] \n",
    "    insult_labels = test_labels.iloc[0:num_to_use,6:7] \n",
    "    identity_hate_labels = test_labels.iloc[0:num_to_use,7:8] \n",
    "    \n",
    "    return toxic_train_labels, severe_toxic_labels, obscene_labels, threat_labels, insult_labels, identity_hate_labels\n",
    "\n",
    "def train_model(labels, features):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(features, labels)\n",
    "    score = model.score(features, labels)\n",
    "    return score, model\n",
    "\n",
    "count_features_train, count_features_test, count_vectorizer = vectorize_count()\n",
    "tfidf_features_train, tfidf_features_test, tfidf_vectorizer = vectorize_tfidf()\n",
    "\n",
    "toxic_train_labels, severe_toxic_labels, obscene_labels, threat_labels, insult_labels, identity_hate_labels = get_test_labels_as_cols(len(train), train)\n",
    "\n",
    "#toxic\n",
    "#train model, then get labels for test data\n",
    "print(\"toxic\")\n",
    "score_toxic, model = train_model(toxic_train_labels.values.ravel(), count_features_train)\n",
    "toxic_labels = model.predict(count_features_test)\n",
    "\n",
    "# severe_toxic_labels\n",
    "print(\"severe_toxic\")\n",
    "score_severe_toxic, model = train_model(severe_toxic_labels.values.ravel(), tfidf_features_train)\n",
    "severe_toxic_labels = model.predict(tfidf_features_test)\n",
    "\n",
    "\n",
    "# #obscene_labels\n",
    "print(\"obscene\")\n",
    "score_obscene, model = train_model(obscene_labels.values.ravel(), count_features_train)\n",
    "obscene_labels = model.predict(count_features_test)\n",
    "\n",
    "#threat_labels\n",
    "print(\"threat\")\n",
    "score_threat, model = train_model(threat_labels.values.ravel(), tfidf_features_train)\n",
    "threat_labels = model.predict(tfidf_features_test)\n",
    "\n",
    "#insult_labels\n",
    "print(\"insult\")\n",
    "score_insult, model = train_model(insult_labels.values.ravel(), count_features_train)\n",
    "insult_labels = model.predict(count_features_test)\n",
    "\n",
    "# identity_hate_labels\n",
    "print(\"identity_hate\")\n",
    "score_identity_hate, model = train_model(identity_hate_labels.values.ravel(), tfidf_features_train)\n",
    "identity_hate_labels = model.predict(tfidf_features_test)\n",
    "\n",
    "\n",
    "#combine all labels\n",
    "labels = pd.DataFrame({'toxic': toxic_labels, 'severe_toxic': severe_toxic_labels, 'obscene': obscene_labels, 'threat': threat_labels, 'insult': insult_labels, 'identity_hate': identity_hate_labels})\n",
    "print(labels.head)\n",
    "print(len(labels))\n",
    "print(\"labels: \" + str(test_labels[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num predicted labels: 63978\n",
      "num actual labels: 63978\n",
      "toxic_accuracy: 0.9322892244208947\n",
      "severe_toxic_accuracy: 0.9942636531307637\n",
      "obscene_accuracy: 0.9572352996342492\n",
      "threat_accuracy: 0.9967019913095126\n",
      "insult_accuracy: 0.954453093250805\n",
      "identity_hate_accuracy: 0.9888711744662227\n"
     ]
    }
   ],
   "source": [
    "#calculate accuracy\n",
    "print(\"num predicted labels: \" + str(len(labels)))\n",
    "print(\"num actual labels: \" + str(len(test_labels)))\n",
    "\n",
    "#drop indices so we can compare \n",
    "labels = labels.reset_index(drop=True)\n",
    "test_labels = test_labels.reset_index(drop=True)\n",
    "\n",
    "total = len(res)\n",
    "\n",
    "#toxic\n",
    "res = np.where(labels['toxic'] == test_labels['toxic'], 1, 0)\n",
    "count_correct = sum(res)\n",
    "toxic_accuracy = count_correct/total\n",
    "print(\"toxic_accuracy: \" + str(toxic_accuracy))\n",
    "\n",
    "#severe_toxic\n",
    "res = np.where(labels['severe_toxic'] == test_labels['severe_toxic'], 1, 0)\n",
    "count_correct = sum(res)\n",
    "accuracy = count_correct/total\n",
    "print(\"severe_toxic_accuracy: \" + str(accuracy))\n",
    "\n",
    "#obscene\n",
    "res = np.where(labels['obscene'] == test_labels['obscene'], 1, 0)\n",
    "count_correct = sum(res)\n",
    "accuracy = count_correct/total\n",
    "print(\"obscene_accuracy: \" + str(accuracy))\n",
    "\n",
    "#threat\n",
    "res = np.where(labels['threat'] == test_labels['threat'], 1, 0)\n",
    "count_correct = sum(res)\n",
    "accuracy = count_correct/total\n",
    "print(\"threat_accuracy: \" + str(accuracy))\n",
    "\n",
    "#insult\n",
    "res = np.where(labels['insult'] == test_labels['insult'], 1, 0)\n",
    "count_correct = sum(res)\n",
    "accuracy = count_correct/total\n",
    "print(\"insult_accuracy: \" + str(accuracy))\n",
    "\n",
    "#identity hate\n",
    "res = np.where(labels['identity_hate'] == test_labels['identity_hate'], 1, 0)\n",
    "count_correct = sum(res)\n",
    "accuracy = count_correct/total\n",
    "print(\"identity_hate_accuracy: \" + str(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "0.9753150635140471\n",
      "severe_toxic\n",
      "0.9899417814013824\n",
      "obscene\n",
      "0.9827976261350747\n",
      "threat\n",
      "0.9969856678218473\n",
      "insult\n",
      "0.9793195505448985\n",
      "identity_hate\n",
      "0.9911387407486323\n"
     ]
    }
   ],
   "source": [
    "#train model on all data (including the labeled test data + given train data)\n",
    "#run test data: we will use tfidf for severe toxic, threat, and identity hate\n",
    "#and we will use countvectorizer: less severe labels, i.e. toxic, obscene, and insult.\n",
    "def get_data(num_to_use, data): \n",
    "    comments = data.iloc[0:num_to_use,1]\n",
    "    return comments\n",
    "\n",
    "test_comments = get_data(len(test), test)\n",
    "# print(test_comments)\n",
    "train_comments = get_data(len(train), train)\n",
    "# print(train_comments)\n",
    "\n",
    "#combine the comments \n",
    "comments = pd.concat([train_comments, test_comments])\n",
    "# print(comments)\n",
    "\n",
    "# #combine the labels: drop indices \n",
    "# print(len(train) + len(test_labels))\n",
    "train_labels = train.reset_index(drop=True)\n",
    "test_labels = test_labels.reset_index(drop=True)\n",
    "labels = pd.concat([train_labels, test_labels], sort=False)\n",
    "# print(len(labels))\n",
    "\n",
    "def vectorize_tfidf(): \n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, stop_words='english')\n",
    "    features_train = vectorizer.fit_transform(train_comments)\n",
    "    features_test = vectorizer.transform(test_comments)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "#     print(train_vocab)\n",
    "    return features_train, features_test, vectorizer\n",
    "\n",
    "def vectorize_count():\n",
    "    vectorizer = CountVectorizer(max_df=0.5, stop_words='english', ngram_range=(1,2))\n",
    "    features_train = vectorizer.fit_transform(train_comments)\n",
    "    features_test = vectorizer.transform(test_comments)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "#     print(train_vocab)\n",
    "    return features_train, features_test, vectorizer\n",
    "\n",
    "def get_test_labels_as_cols(num_to_use, test_labels):\n",
    "    #labels as pd dataframes\n",
    "    toxic_train_labels = test_labels.iloc[0:num_to_use,2:3]\n",
    "    severe_toxic_labels = test_labels.iloc[0:num_to_use,3:4] \n",
    "    obscene_labels = test_labels.iloc[0:num_to_use,4:5] \n",
    "    threat_labels = test_labels.iloc[0:num_to_use,5:6] \n",
    "    insult_labels = test_labels.iloc[0:num_to_use,6:7] \n",
    "    identity_hate_labels = test_labels.iloc[0:num_to_use,7:8] \n",
    "    \n",
    "    return toxic_train_labels, severe_toxic_labels, obscene_labels, threat_labels, insult_labels, identity_hate_labels\n",
    "\n",
    "def train_model(labels, features):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(features, labels)\n",
    "    score = model.score(features, labels)\n",
    "    return score, model\n",
    "\n",
    "count_features_train, count_features_test, count_vectorizer = vectorize_count()\n",
    "tfidf_features_train, tfidf_features_test, tfidf_vectorizer = vectorize_tfidf()\n",
    "\n",
    "toxic_train_labels, severe_toxic_labels, obscene_labels, threat_labels, insult_labels, identity_hate_labels = get_test_labels_as_cols(len(train), train)\n",
    "\n",
    "#toxic\n",
    "#train model, then get labels for test data\n",
    "print(\"toxic\")\n",
    "score, model_toxic = train_model(toxic_train_labels.values.ravel(), count_features_train)\n",
    "toxic_labels = model_toxic.predict(count_features_test)\n",
    "print(score)\n",
    "\n",
    "# severe_toxic_labels\n",
    "print(\"severe_toxic\")\n",
    "score, model_severe = train_model(severe_toxic_labels.values.ravel(), tfidf_features_train)\n",
    "severe_toxic_labels = model_severe.predict(tfidf_features_test)\n",
    "print(score)\n",
    "\n",
    "\n",
    "# #obscene_labels\n",
    "print(\"obscene\")\n",
    "score, model_obscene = train_model(obscene_labels.values.ravel(), count_features_train)\n",
    "obscene_labels = model_obscene.predict(count_features_test)\n",
    "print(score)\n",
    "\n",
    "#threat_labels\n",
    "print(\"threat\")\n",
    "score, model_threat = train_model(threat_labels.values.ravel(), tfidf_features_train)\n",
    "threat_labels = model_threat.predict(tfidf_features_test)\n",
    "print(score)\n",
    "\n",
    "#insult_labels\n",
    "print(\"insult\")\n",
    "score, model_insult = train_model(insult_labels.values.ravel(), count_features_train)\n",
    "insult_labels = model_insult.predict(count_features_test)\n",
    "print(score)\n",
    "\n",
    "# identity_hate_labels\n",
    "print(\"identity_hate\")\n",
    "score, model_identity_hate = train_model(identity_hate_labels.values.ravel(), tfidf_features_train)\n",
    "identity_hate_labels = model_identity_hate.predict(tfidf_features_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "severe_toxic\n",
      "obscene\n",
      "threat\n",
      "insult\n",
      "identity_hate\n",
      "<bound method NDFrame.head of        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
      "0          0             0        0       0       0              0\n",
      "1          0             0        0       0       0              0\n",
      "2          0             0        0       0       0              0\n",
      "3          0             0        0       0       0              0\n",
      "4          0             0        0       0       0              0\n",
      "5          0             0        0       0       0              0\n",
      "6          0             0        0       0       0              0\n",
      "7          0             0        0       0       0              0\n",
      "8          0             0        0       0       0              0\n",
      "9          0             0        0       0       0              0\n",
      "10         0             0        0       0       0              0\n",
      "11         0             0        0       0       0              0\n",
      "12         0             0        0       0       0              0\n",
      "13         0             0        0       0       0              0\n",
      "14         0             0        0       0       0              0\n",
      "15         0             0        0       0       0              0\n",
      "16         0             0        0       0       0              0\n",
      "17         0             0        0       0       0              0\n",
      "18         0             0        0       0       0              0\n",
      "19         0             0        0       0       0              0\n",
      "20         0             0        0       0       0              0\n",
      "21         0             0        0       0       0              0\n",
      "22         0             0        0       0       0              0\n",
      "23         0             0        0       0       0              0\n",
      "24         0             0        0       0       0              0\n",
      "25         0             0        0       0       0              0\n",
      "26         0             0        0       0       0              0\n",
      "27         0             0        0       0       0              0\n",
      "28         0             0        0       0       0              0\n",
      "29         0             0        0       0       0              0\n",
      "...      ...           ...      ...     ...     ...            ...\n",
      "19250      0             0        0       0       0              0\n",
      "19251      0             0        0       0       0              0\n",
      "19252      0             0        0       0       0              0\n",
      "19253      0             0        0       0       0              0\n",
      "19254      0             0        0       0       0              0\n",
      "19255      0             0        0       0       0              0\n",
      "19256      0             0        0       0       0              0\n",
      "19257      0             0        0       0       0              0\n",
      "19258      1             0        0       0       1              0\n",
      "19259      0             0        0       0       0              0\n",
      "19260      0             0        0       0       0              0\n",
      "19261      0             0        0       0       0              0\n",
      "19262      0             0        0       0       0              0\n",
      "19263      0             0        0       0       0              0\n",
      "19264      0             0        0       0       0              0\n",
      "19265      0             0        0       0       0              0\n",
      "19266      0             0        0       0       0              0\n",
      "19267      0             0        0       0       0              0\n",
      "19268      0             0        0       0       0              0\n",
      "19269      0             0        0       0       0              0\n",
      "19270      0             0        0       0       0              0\n",
      "19271      0             0        0       0       0              0\n",
      "19272      0             0        0       0       0              0\n",
      "19273      0             0        0       0       0              0\n",
      "19274      0             0        0       0       0              0\n",
      "19275      0             0        0       0       0              0\n",
      "19276      0             0        0       0       0              0\n",
      "19277      0             0        0       0       0              0\n",
      "19278      0             0        0       0       0              0\n",
      "19279      0             0        0       0       0              0\n",
      "\n",
      "[19280 rows x 6 columns]>\n",
      "19280\n",
      "19280\n",
      "70\n",
      "0\n",
      "20\n",
      "0\n",
      "18\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#load tweets and find the toxic tweets\n",
    "tweet_path = \"./data/tweetsUsers.csv\"\n",
    "\n",
    "#load data with pandas and return dataframes \n",
    "def load_tweets(tweet_path):\n",
    "    tweets = pd.read_csv(tweet_path)\n",
    "    comments = tweets['text']\n",
    "    users = tweets['screen_name']\n",
    "    return comments, users\n",
    "\n",
    "tweets, users = load_tweets(tweet_path)\n",
    "\n",
    "# print(tweets[0:10])\n",
    "\n",
    "def vectorize_tfidf(docs): \n",
    "    features = tfidf_vectorizer.transform(docs)\n",
    "    return features\n",
    "\n",
    "def vectorize_count(docs):\n",
    "    features = count_vectorizer.transform(docs)\n",
    "    return features\n",
    "\n",
    "count_features_test = vectorize_count(tweets)\n",
    "tfidf_features_test = vectorize_tfidf(tweets)\n",
    "\n",
    "#toxic\n",
    "#train model, then get labels for test data\n",
    "print(\"toxic\")\n",
    "toxic_labels = model_toxic.predict(count_features_test)\n",
    "\n",
    "# severe_toxic_labels\n",
    "print(\"severe_toxic\")\n",
    "severe_toxic_labels = model_severe.predict(tfidf_features_test)\n",
    "\n",
    "# #obscene_labels\n",
    "print(\"obscene\")\n",
    "obscene_labels = model_obscene.predict(count_features_test)\n",
    "\n",
    "#threat_labels\n",
    "print(\"threat\")\n",
    "threat_labels = model_threat.predict(tfidf_features_test)\n",
    "\n",
    "#insult_labels\n",
    "print(\"insult\")\n",
    "insult_labels = model_insult.predict(count_features_test)\n",
    "\n",
    "# identity_hate_labels\n",
    "print(\"identity_hate\")\n",
    "identity_hate_labels = model_identity_hate.predict(tfidf_features_test)\n",
    "    \n",
    "#combine labels\n",
    "labels = pd.DataFrame({'toxic': toxic_labels, 'severe_toxic': severe_toxic_labels, 'obscene': obscene_labels, 'threat': threat_labels, 'insult': insult_labels, 'identity_hate': identity_hate_labels})\n",
    "print(labels.head)\n",
    "print(len(labels))\n",
    "print(len(tweets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3716     realDonaldTrump\n",
      "7460          TomiLahren\n",
      "8069          TomiLahren\n",
      "8280          TomiLahren\n",
      "8565          TomiLahren\n",
      "9456          TomiLahren\n",
      "10540         AnnCoulter\n",
      "12076         AnnCoulter\n",
      "12217         AnnCoulter\n",
      "12746       MADENAMERUCA\n",
      "12770       MADENAMERUCA\n",
      "12911      LastStand2019\n",
      "13260      LastStand2019\n",
      "13387      LastStand2019\n",
      "13707      LastStand2019\n",
      "13756      LastStand2019\n",
      "14154      LastStand2019\n",
      "14157      LastStand2019\n",
      "14750      LastStand2019\n",
      "14873      LastStand2019\n",
      "15717      LastStand2019\n",
      "15762      LastStand2019\n",
      "15854      LastStand2019\n",
      "16147             Mecdty\n",
      "16271             Mecdty\n",
      "16408             Mecdty\n",
      "16414             Mecdty\n",
      "16471             Mecdty\n",
      "16499             Mecdty\n",
      "16541             Mecdty\n",
      "              ...       \n",
      "17385             Mecdty\n",
      "17397             Mecdty\n",
      "17487             Mecdty\n",
      "17509             Mecdty\n",
      "17777             Mecdty\n",
      "17837             Mecdty\n",
      "17987             Mecdty\n",
      "18024             Mecdty\n",
      "18067             Mecdty\n",
      "18131             Mecdty\n",
      "18144             Mecdty\n",
      "18145             Mecdty\n",
      "18207             Mecdty\n",
      "18217             Mecdty\n",
      "18346             Mecdty\n",
      "18432             Mecdty\n",
      "18453             Mecdty\n",
      "18492             Mecdty\n",
      "18533             Mecdty\n",
      "18581             Mecdty\n",
      "18587             Mecdty\n",
      "18775             Mecdty\n",
      "18897             Mecdty\n",
      "18907             Mecdty\n",
      "19123             Mecdty\n",
      "19135             Mecdty\n",
      "19163             Mecdty\n",
      "19214             Mecdty\n",
      "19247             Mecdty\n",
      "19258             Mecdty\n",
      "Name: screen_name, Length: 70, dtype: object\n",
      "                 users                                             tweets\n",
      "3716   realDonaldTrump             A total loser! https://t.co/vm3Vv2f9jf\n",
      "7460        TomiLahren  Love him or hate him, he’s still their obsessi...\n",
      "8069        TomiLahren  Hey guess what, you can still pout in your own...\n",
      "8280        TomiLahren     @TomiLahren Uhhhh @chrissyteigen / @johnlegend\n",
      "8565        TomiLahren  Millions of taxpayer dollars and still NO EVID...\n",
      "9456        TomiLahren  Well ain’t that some shit. https://t.co/QCXWdi...\n",
      "10540       AnnCoulter     \"fuck off I'm talking\" https://t.co/GDqCTOBpvQ\n",
      "12076       AnnCoulter      The Pats were like \"ok, enough of this shit.\"\n",
      "12217       AnnCoulter  \"ignorant bastard, THROW THE FUCKING STICK\" ht...\n",
      "12746     MADENAMERUCA  @mel_faith1 If that were my kid, my foot would...\n",
      "12770     MADENAMERUCA  @ChatByCC @realDonaldTrump @steph93065 @Rockpr...\n",
      "12911    LastStand2019  Mines not for sale....but you can kiss my ass ...\n",
      "13260    LastStand2019  @LastStand2019 @BreitbartNews She needs to shu...\n",
      "13387    LastStand2019  Kamala Harris flip-flopping like Mitt Romney i...\n",
      "13707    LastStand2019  Bill Kristol is a sociopath. Closet commie. 10...\n",
      "13756    LastStand2019  @LastStand2019 Go sniff a hairy smelly BUTT , ...\n",
      "14154    LastStand2019  @LastStand2019 @realDonaldTrump I say hell no,...\n",
      "14157    LastStand2019  @LastStand2019 @realDonaldTrump That’s typical...\n",
      "14750    LastStand2019  @LastStand2019 @BreitbartNews BLT and I would ...\n",
      "14873    LastStand2019  Mines not for sale....but you can kiss my ass ...\n",
      "15717    LastStand2019  @LastStand2019 A BIG FAT... https://t.co/X3664...\n",
      "15762    LastStand2019          @LastStand2019 She is a complete imbecile\n",
      "15854    LastStand2019             @LastStand2019 Man, I hate that bitch.\n",
      "16147           Mecdty  @MarkDice I’d rather get kicked in the Nuts th...\n",
      "16271           Mecdty  @HillaryClinton Coming from the mouth of the m...\n",
      "16408           Mecdty  @Jamierodr14 @realDonaldTrump @DevinNunes <U+2...\n",
      "16414           Mecdty  @Jenn198523 @Emaxx2044 They are the peaceful p...\n",
      "16471           Mecdty  https://t.co/K7AmxPcBJ4\\r\\n\\r\\nNOW he wants to...\n",
      "16499           Mecdty  @DeepStateExpose People need to fight back, if...\n",
      "16541           Mecdty  @BarackObama Liar! You don’t give a Rats Ass! ...\n",
      "...                ...                                                ...\n",
      "17385           Mecdty  @MAGA2ARIGHTS @gatewaypundit You can put lipst...\n",
      "17397           Mecdty  @RealTrumpLady @gatewaypundit You can dress a ...\n",
      "17487           Mecdty  @The_Trump_Train <U+270B><U+270B><U+270B><U+27...\n",
      "17509           Mecdty  @SuperEliteTexan Fuk them! Assimilate or get t...\n",
      "17777           Mecdty     @collin_tuso @GKeile A bunch of Dumb Ass Kids!\n",
      "17837           Mecdty  @SenBlumenthal Liar! Your parents give you the...\n",
      "17987           Mecdty                         @RealMattCouch Noooooooooo\n",
      "18024           Mecdty  @Amy_Siskind Take your Bra Off get some popcor...\n",
      "18067           Mecdty  @thebradfordfile More like a Pig in a Pig Clot...\n",
      "18131           Mecdty  @adriandt31 @quickslanding Damn the ugly stick...\n",
      "18144           Mecdty  Lolol I called Omar a Racist Bitch and Tweeter...\n",
      "18145           Mecdty  @IlhanMN Racist Bitch! You have No morals! Dis...\n",
      "18207           Mecdty                 @BillKristol Your such an Asshole!\n",
      "18217           Mecdty  @bud_cann @PhysEdDepot Fukking Idiots! They sh...\n",
      "18346           Mecdty           @chrissyteigen Fukking disgusting bitch!\n",
      "18432           Mecdty  @AOC Ahhhh maybe because you are a Fukking idi...\n",
      "18453           Mecdty  @jamesplake721 <U+270B><U+270B><U+270B><U+270B...\n",
      "18492           Mecdty  @AOC @jacobsoboroff If you haven’t figured you...\n",
      "18533           Mecdty  @THEKIDMERO @TuckerCarlson @chrislhayes Wow yo...\n",
      "18581           Mecdty                                    @IlhanMN Idiot!\n",
      "18587           Mecdty                  @AOCpress Make me a Drink! Bitch!\n",
      "18775           Mecdty   @ThatTrumpGuy I’d knock that Bitches Lights Out!\n",
      "18897           Mecdty             @realDonaldTrump Shut down the Boarder\n",
      "18907           Mecdty  @thehill See ya! Who gives a Crap what these m...\n",
      "19123           Mecdty  @DuckTvvacy @JannaWilkinso69 I wouldn’t eat an...\n",
      "19135           Mecdty                        Smollett is a Fukking Liar!\n",
      "19163           Mecdty                          @robreiner STFU you Liar!\n",
      "19214           Mecdty  Breaking ...\\r\\n\\r\\nHillary has just released ...\n",
      "19247           Mecdty  @ericswalwell Liar, Liar, Liar, Liar, Liar, Li...\n",
      "19258           Mecdty  @ericswalwell Liar, Liar, Liar, Liar, Liar, Li...\n",
      "\n",
      "[70 rows x 2 columns]\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "#data analysis\n",
    "\n",
    "num_toxic = sum(np.where(labels['toxic'] == 1, 1, 0))\n",
    "toxic = labels.index[labels['toxic'] == 1].tolist()\n",
    "# print(tweets.loc[toxic])\n",
    "# print(users.loc[toxic])\n",
    "print(users.loc[toxic])\n",
    "df = pd.DataFrame({'users': users.loc[toxic], 'tweets': tweets.loc[toxic]})\n",
    "print(df)\n",
    "df.to_csv(\"data/toxic_tweets.csv\")\n",
    "print(num_toxic)\n",
    "\n",
    "# num_toxic = sum(np.where(labels['severe_toxic'] == 1, 1, 0))\n",
    "# print(num_toxic)\n",
    "\n",
    "# num_toxic = sum(np.where(labels['obscene'] == 1, 1, 0))\n",
    "# print(num_toxic)\n",
    "\n",
    "# num_toxic = sum(np.where(labels['threat'] == 1, 1, 0))\n",
    "# print(num_toxic)\n",
    "\n",
    "# num_toxic = sum(np.where(labels['insult'] == 1, 1, 0))\n",
    "# print(num_toxic)\n",
    "\n",
    "# num_toxic = sum(np.where(labels['identity_hate'] == 1, 1, 0))\n",
    "# print(num_toxic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# from langdetect import detect\n",
    "# import re\n",
    "\n",
    "# sentence = \"\"\n",
    "# vocab = list(vectorizer.vocabulary_.keys())\n",
    "# cut = int(3.0 * len(vocab) / 4.0)\n",
    "# vocab = vocab[:len(vocab) - cut]\n",
    "# sentenceLength = int(random.random() * 8) + 3\n",
    "# for i in range(sentenceLength):\n",
    "#     word = \"\"\n",
    "#     isEnglish = False\n",
    "#     while not isEnglish:\n",
    "#         falloff = 5\n",
    "#         sample = random.random() ** falloff\n",
    "#         index = int(sample * len(vocab))\n",
    "#         word = vocab[index]\n",
    "#         word = \"\".join(re.split(\"[^a-zA-Z]*\", word))\n",
    "#         if word == \"\":\n",
    "#             continue\n",
    "#         isEnglish = (detect(word) == \"en\")\n",
    "    \n",
    "#     sentence += \" \" + word\n",
    "    \n",
    "# print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
