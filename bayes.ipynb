{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
      "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
      "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
      "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
      "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n",
      "5             0        0       0       0              0  \n",
      "6             1        1       0       1              0  \n",
      "7             0        0       0       0              0  \n",
      "8             0        0       0       0              0  \n",
      "9             0        0       0       0              0  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# #check file paths for data paths \n",
    "# from os import listdir\n",
    "\n",
    "# #list files/directories in current directory\n",
    "# for file in listdir(\"./\"):\n",
    "#     print(file)\n",
    "    \n",
    "train_path = \"./data/train.csv\"\n",
    "test_path = \"./data/test.csv\"\n",
    "test_labels_path = \"./data/test_labels.csv\"\n",
    "\n",
    "#load data with pandas and return dataframes \n",
    "def load_data(train_path, test_path, test_labels_path):\n",
    "    train = pd.read_csv(train_path)\n",
    "    print(train[0:10])\n",
    "    test = pd.read_csv(test_path)\n",
    "    test_labels = pd.read_csv(test_labels_path)\n",
    "    cols = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']\n",
    "    return train, test, test_labels, cols\n",
    "\n",
    "\n",
    "train, test, test_labels, cols = load_data(train_path, test_path, test_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize data\n",
    "def summary():\n",
    "    labels = train.iloc[:,2:]\n",
    "    zero = np.where(np.sum(labels,axis=1)==0)\n",
    "    # Find the unlabelled percentage\n",
    "    unlabelled = train[(train['toxic']!=1) & (train['severe_toxic']!=1) &\\\n",
    "                                 (train['obscene']!=1) & (train['threat']!=1) &\\\n",
    "                                 (train['insult']!=1) & (train['identity_hate']!=1)]\n",
    "    print(\"Train data length: \", len(train))\n",
    "    print(\"Test data length: \", len(test))\n",
    "    print('\\nPercentage of unlabelled: ', len(unlabelled)/len(train)*100)\n",
    "    print(train[cols].sum())\n",
    "\n",
    "    train[cols].sum().plot.bar(title =\"Number of Comments by Label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(num_to_use): \n",
    "    #temporarily only use 500 \n",
    "    train_comments = train.iloc[0:num_to_use,1]\n",
    "    test_comments = test.iloc[0:num_to_use,1]\n",
    "    \n",
    "    return test_comments, train_comments\n",
    "\n",
    "test_comments, train_comments= get_data(len(train))\n",
    "\n",
    "\n",
    "# each label as a single combined string, e.g. toxic & severe_toxic = 11\n",
    "def get_labels_as_vec(num_to_use):\n",
    "    #labels as numpy \n",
    "    labels = train.iloc[0:num_to_use,2:]\n",
    "    labels['vector_label'] = labels.astype(str).values.sum(axis=1)\n",
    "    train_labels = labels['vector_label']\n",
    "    \n",
    "    return train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternate : each label as a separate pd column\n",
    "def get_labels_as_cols(num_to_use):\n",
    "    #labels as pd dataframes\n",
    "    toxic_train_labels = train.iloc[0:num_to_use,2:3]\n",
    "    severe_toxic_labels = train.iloc[0:num_to_use,3:4] \n",
    "    obscene_labels = train.iloc[0:num_to_use,4:5] \n",
    "    threat_labels = train.iloc[0:num_to_use,5:6] \n",
    "    insult_labels = train.iloc[0:num_to_use,6:7] \n",
    "    identity_hate_labels = train.iloc[0:num_to_use,7:8] \n",
    "    \n",
    "    return toxic_train_labels, severe_toxic_labels, obscene_labels, threat_labels, insult_labels, identity_hate_labels\n",
    "\n",
    "toxic_train_labels, severe_toxic_labels, obscene_labels, threat_labels, insult_labels, identity_hate_labels = get_labels_as_cols(len(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize inputs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def vectorize_tfidf():\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, stop_words='english', ngram_range=(1,2))\n",
    "    features_train = vectorizer.fit_transform(train_comments)\n",
    "    features_test = vectorizer.fit_transform(test_comments)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "#     print(train_vocab)\n",
    "    return features_train, features_test, vectorizer\n",
    "\n",
    "def vectorize_count():\n",
    "    vectorizer = CountVectorizer(max_df=0.5, stop_words='english', ngram_range=(1, 2))\n",
    "    features_train = vectorizer.fit_transform(train_comments)\n",
    "    features_test = vectorizer.fit_transform(test_comments)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "#     print(train_vocab)\n",
    "    return features_train, features_test, vectorizer\n",
    "\n",
    "features_train, features_test, vectorizer = vectorize_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import make_scorer, recall_score, precision_score, accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#Multinomial NB\n",
    "def train_model(labels):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(features_train, labels)\n",
    "    score = model.score(features_train, labels)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_avg_cvd_score(model, X, y):\n",
    "    to_get = { 'accuracy' : make_scorer(accuracy_score) }#,\n",
    "   # 'precision' : make_scorer(precision_score, average='macro'),\n",
    "    #'recall' : make_scorer(recall_score, average='macro')}\n",
    "    results = cross_validate(model, X, y, scoring=to_get, cv=5)\n",
    "\n",
    "    return np.average(results['test_accuracy'])#, np.average(results['test_precision']), np.average(results['test_recall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toxic\n",
    "score_toxic = train_model(toxic_train_labels.values.ravel())\n",
    "toxic_accuracy = get_avg_cvd_score(MultinomialNB(), features_train, toxic_train_labels.values.ravel())\n",
    "# toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, toxic_train_labels.values.ravel())\n",
    "print(\"toxic\")\n",
    "print(\"training accuracy: \" + str(score_toxic))\n",
    "print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "# print(\"precision: \" +str(toxic_precision))\n",
    "# print(\"recall: \" +str(toxic_recall))\n",
    "print(\"\\n\")\n",
    "\n",
    "#severe_toxic_labels\n",
    "print(\"severe_toxic\")\n",
    "score_severe_toxic = train_model(severe_toxic_labels.values.ravel())\n",
    "toxic_accuracy =  get_avg_cvd_score(MultinomialNB(), features_train, severe_toxic_labels.values.ravel())\n",
    "# toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, severe_toxic_labels.values.ravel())\n",
    "print(score_severe_toxic)\n",
    "print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "# print(\"precision: \" +str(toxic_precision))\n",
    "# print(\"recall: \" +str(toxic_recall))\n",
    "print(\"\\n\")\n",
    "\n",
    "#obscene_labels\n",
    "print(\"obscene\")\n",
    "score_obscene = train_model(obscene_labels.values.ravel())\n",
    "toxic_accuracy =  get_avg_cvd_score(MultinomialNB(), features_train, obscene_labels.values.ravel())\n",
    "# toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, obscene_labels.values.ravel())\n",
    "print(score_obscene)\n",
    "print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "# print(\"precision: \" +str(toxic_precision))\n",
    "# print(\"recall: \" +str(toxic_recall))\n",
    "print(\"\\n\")\n",
    "\n",
    "#threat_labels\n",
    "print(\"threat\")\n",
    "score_threat = train_model(threat_labels.values.ravel())\n",
    "toxic_accuracy = get_avg_cvd_score(MultinomialNB(), features_train, threat_labels.values.ravel())\n",
    "#toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, threat_labels.values.ravel())\n",
    "print(score_threat)\n",
    "print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "# print(\"precision: \" +str(toxic_precision))\n",
    "# print(\"recall: \" +str(toxic_recall))\n",
    "print(\"\\n\")\n",
    "\n",
    "#insult_labels\n",
    "print(\"insult\")\n",
    "score_insult = train_model(insult_labels.values.ravel())\n",
    "toxic_accuracy = get_avg_cvd_score(MultinomialNB(), features_train, insult_labels.values.ravel())\n",
    "# toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, insult_labels.values.ravel())\n",
    "print(score_insult)\n",
    "print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "# print(\"precision: \" +str(toxic_precision))\n",
    "# print(\"recall: \" +str(toxic_recall))\n",
    "print(\"\\n\")\n",
    "\n",
    "#identity_hate_labels\n",
    "print(\"identity_hate\")\n",
    "score_identity_hate = train_model(identity_hate_labels.values.ravel())\n",
    "toxic_accuracy = get_avg_cvd_score(MultinomialNB(), features_train, identity_hate_labels.values.ravel())\n",
    "# toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, identity_hate_labels.values.ravel())\n",
    "print(score_identity_hate)\n",
    "print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "# print(\"precision: \" +str(toxic_precision))\n",
    "# print(\"recall: \" +str(toxic_recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import make_scorer, recall_score, precision_score, accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#Multinomial NB\n",
    "def train_model(labels):\n",
    "    model = GaussianNB()\n",
    "    model.fit(features_train, labels)\n",
    "    score = model.score(features_train, labels)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_avg_cvd_score(model, X, y):\n",
    "    to_get = { 'accuracy' : make_scorer(accuracy_score),\n",
    "    'precision' : make_scorer(precision_score, average='macro'),\n",
    "    'recall' : make_scorer(recall_score, average='macro')}\n",
    "    results = cross_validate(model, X, y, scoring=to_get, cv=5)\n",
    "\n",
    "    return np.average(results['test_accuracy']), np.average(results['test_precision']), np.average(results['test_recall'])\n",
    "\n",
    "#change to array\n",
    "features_train = features_train.toarray()\n",
    "#toxic\n",
    "score = train_model(toxic_train_labels.values.ravel())\n",
    "toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, toxic_train_labels.values.ravel())\n",
    "print(\"toxic\")\n",
    "print(\"training accuracy: \" + str(score_toxic))\n",
    "print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "print(\"precision: \" +str(toxic_precision))\n",
    "print(\"recall: \" +str(toxic_recall))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# #severe_toxic_labels\n",
    "# print(\"severe_toxic\")\n",
    "# score_severe_toxic = train_model(severe_toxic_labels.values.ravel())\n",
    "# toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, severe_toxic_labels.values.ravel())\n",
    "# print(score_severe_toxic)\n",
    "# print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "# print(\"precision: \" +str(toxic_precision))\n",
    "# print(\"recall: \" +str(toxic_recall))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# #obscene_labels\n",
    "# print(\"obscene\")\n",
    "# score_obscene = train_model(obscene_labels.values.ravel())\n",
    "# toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, obscene_labels.values.ravel())\n",
    "# print(score_obscene)\n",
    "# print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "# print(\"precision: \" +str(toxic_precision))\n",
    "# print(\"recall: \" +str(toxic_recall))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# #threat_labels\n",
    "# print(\"threat\")\n",
    "# score_threat = train_model(threat_labels.values.ravel())\n",
    "# toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, threat_labels.values.ravel())\n",
    "# print(score_threat)\n",
    "# print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "# print(\"precision: \" +str(toxic_precision))\n",
    "# print(\"recall: \" +str(toxic_recall))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# #insult_labels\n",
    "# print(\"insult\")\n",
    "# score_insult = train_model(insult_labels.values.ravel())\n",
    "# toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, insult_labels.values.ravel())\n",
    "# print(score_insult)\n",
    "# print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "# print(\"precision: \" +str(toxic_precision))\n",
    "# print(\"recall: \" +str(toxic_recall))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# #identity_hate_labels\n",
    "# print(\"identity_hate\")\n",
    "# score_identity_hate = train_model(identity_hate_labels.values.ravel())\n",
    "# toxic_accuracy, toxic_precision, toxic_recall = get_avg_cvd_score(MultinomialNB(), features_train, identity_hate_labels.values.ravel())\n",
    "# print(score_identity_hate)\n",
    "# print(\"validation accuracy: \" + str(toxic_accuracy))\n",
    "# print(\"precision: \" +str(toxic_precision))\n",
    "# print(\"recall: \" +str(toxic_recall))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
