{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from nltk import TweetTokenizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "tweet_path = \"./data/tweetsUsers.csv\"\n",
    "\n",
    "#load data with pandas and return dataframes (converted to numpy arrays)\n",
    "def load_tweets(tweet_path):\n",
    "    tweets = pd.read_csv(tweet_path)\n",
    "    \n",
    "    #choose first 100 for each user for testing purposes\n",
    "    tweets = tweets.sort_values('screen_name').groupby('screen_name').head(100)\n",
    "    comments = tweets['text']\n",
    "    users = tweets['screen_name']\n",
    "    return comments.to_numpy(), users.to_numpy()\n",
    "\n",
    "tweets, users = load_tweets(tweet_path)\n",
    "\n",
    "tweets = tweets[0:500] #use first 500 for testing\n",
    "users = users[0:500]\n",
    "\n",
    "#remove old style retweets, hashtags and hyperlinks in a string tweet\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet) \n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "#remove punctuation and stopwords in a list of tokens\n",
    "def clean_tweets(tweet_tokens):\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if ((word not in stopwords_english) and (word not in string.punctuation)): # remove punctuation + stopwords\n",
    "            tweets_clean.append(word)\n",
    "    return tweets_clean\n",
    "\n",
    "#tokenize and clean tweets\n",
    "def tokenize(tweets):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True)\n",
    "    count = 0\n",
    "    tokens = [tokenizer.tokenize(clean_tweet(tweet)) for tweet in tweets]\n",
    "    tokens = [clean_tweets(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize(tweets)\n",
    "    \n",
    "# ############################################################\n",
    "# # Extracting and loading data\n",
    "# ############################################################\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.len = len(X)           \n",
    "        if torch.cuda.is_available():\n",
    "          self.x_data = torch.from_numpy(X).float().cuda()\n",
    "          self.y_data = torch.from_numpy(y).long().cuda()\n",
    "        else:\n",
    "          self.x_data = torch.from_numpy(X)\n",
    "          self.y_data = torch.from_numpy(y).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm excited to see @CecileRichards, @aliciagarza, @aijenpoo, and the Pantsuit Nation community build this powerful new force for gender equity. Supermajority's goal is to train and mobilize 2 million women over the next two years. https://t.co/vorkl8sMZG\"\n",
      " 'I condemn this act of domestic terrorism. It’s no accident that hate crimes are on the rise. We cannot turn a blind eye to the fact that the president’s embrace of white nationalist rhetoric, and other politicians’ lack of condemnation of this rhetoric, is fueling these attacks. https://t.co/hNxAsntady'\n",
      " 'So, we did this.  Hillary Clinton Reads the Mueller Report - Klepper https://t.co/3g9tS9uudm via @YouTube'\n",
      " ...\n",
      " 'In Democrat run San Francisco if you don’t clean up your dog’s poop, you’re fined $320.  However, if you defecate on the sidewalk, there’s no fine. You can also get free syringes and use heroin in public. But remember the neither you or your dog are allowed to use plastic straws! https://t.co/Y5CDG6igOG'\n",
      " \"Good Morning fam! It's PUPPY DAY! Today is the day I get my new Pug puppy!\\n\\nHopefully he'll be here by early afternoon! I will take lots of pics. Still thinking of a name, going to wait until I actually MEET him to decide!\"\n",
      " '@RightHookUSA So exciting! <U+0001F495>']\n",
      "['HillaryClinton' 'HillaryClinton' 'HillaryClinton' ... 'TrumpsGAGirl'\n",
      " 'TrumpsGAGirl' 'TrumpsGAGirl']\n",
      "[\"TONIGHT ON #NextRevFNC: Don't miss @SteveHiltonx's guest panel @TomiLahren, @reihan, and @TezlynFigaro! LIVE at 9PM ET on @FoxNews https://t.co/hqJbB11f8z\"\n",
      " \"Woman Gets Uber Driver Fired For Refusing To Take Her To Get An Abortion. Now She's Looking To Sue Him. | Daily Wire https://t.co/7krsMHSgDm\"\n",
      " 'One Potential Russian Collusion Scandal Has Not Yet Been Investigated: Hillary Clinton and Uranium One https://t.co/i6RfEanfTG via @anteksiler'\n",
      " ...\n",
      " 'Looks like we\\'re getting a leadership change at Gitmo...\\n\\n\"due to a loss of confidence in his ability to command...(A spokesman)would not comment on the reasons for Ring\\'s relief or any additional actions that might be taken.\"\\nhttps://t.co/9VB06ojbcx'\n",
      " 'Talking about the Islamic terrorist attacks on September 11th is now \"inciting violence\" against Muslims.\\n\\n#RadicalLeftistLogic https://t.co/5kLSypVzPT via @BreitbartNews'\n",
      " 'I’m so glad we dumped their tea into the harbor and made ourselves our own country https://t.co/J6ESdrOEw2']\n",
      "['TomiLahren' 'TrumpsGAGirl' 'LastStand2019' ... 'TrumpsGAGirl'\n",
      " 'LastStand2019' 'AnnCoulter']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Randomly choose 20 percent of the training data as validation data.\n",
    "\n",
    "    Args:\n",
    "        x_train: training images in shape (num_images,3,image_H,image_W)\n",
    "        y_train: training labels in shape (num_images,)\n",
    "    Returns:\n",
    "        new_x_train: training images in shape (0.8*num_images,3,image_H,image_W)\n",
    "        new_y_train: training labels in shape (0.8*num_images,)\n",
    "        x_val: validation images in shape (0.2*num_images,3,image_H,image_W)\n",
    "        y_val: validation labels in shape (0.2*num_images,)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "    Randomly choose 20 percent of the training data as validation data.\n",
    "\n",
    "    Args:\n",
    "        x_train: training images in shape (num_images,3,image_H,image_W)\n",
    "        y_train: training labels in shape (num_images,)\n",
    "    Returns:\n",
    "        new_x_train: training images in shape (0.8*num_images,3,image_H,image_W)\n",
    "        new_y_train: training labels in shape (0.8*num_images,)\n",
    "        x_val: validation images in shape (0.2*num_images,3,image_H,image_W)\n",
    "        y_val: validation labels in shape (0.2*num_images,)\n",
    "\"\"\"\n",
    "import random\n",
    "import math\n",
    "\n",
    "def create_validation(x_train,y_train):\n",
    "    num_images = len(x_train)\n",
    "    num_validation = math.floor(.2 * num_images)    \n",
    "#     print(num_images)\n",
    "    \n",
    "    #indices of the validation set, random\n",
    "    validation_indices = random.sample(range(0, num_images), num_validation)\n",
    "    \n",
    "    #get the indices of the training set \n",
    "    training_indices = []\n",
    "    for i in range(0, num_images):\n",
    "        if i not in validation_indices:\n",
    "            training_indices.append(i)\n",
    "\n",
    "    x_val = np.take(x_train, validation_indices)\n",
    "    y_val = np.take(y_train, validation_indices)\n",
    "    \n",
    "    new_x_train = np.take(x_train, training_indices)\n",
    "    new_y_train = np.take(y_train, training_indices)\n",
    "        \n",
    "    return new_x_train,new_y_train,x_val,y_val\n",
    "\n",
    "new_x_train, new_y_train, x_val, y_val = create_validation(tweets, users)\n",
    "# print(new_x_train)\n",
    "# print(new_y_train)\n",
    "# print(x_val)\n",
    "# print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Convolutional Neural Network\n",
    "############################################################\n",
    "class ConvolutionalNN(nn.Module):\n",
    "    \"\"\" \n",
    "    (1) Use self.conv1 as the variable name for your first convolutional layer\n",
    "        (2) Use self.pool1 as the variable name for your first pooling layer\n",
    "        (3) Use self.conv2 as the variable name for your second convolutional layer\n",
    "        (4) Use self.pool2 as the variable name for you second pooling layer  \n",
    "        (5) Use self.fc1 as the variable name for your first fully connected layer\n",
    "        (6) Use self.fc2 as the variable name for your second fully connected layer\n",
    "    \"\"\"\n",
    "    \n",
    "     # Hyper-parameters \n",
    "    input_size = 3\n",
    "    hidden_size = 2000\n",
    "    num_classes = 5\n",
    "    num_epochs = 40\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(self.input_size, 16, kernel_size=3, stride=1, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0)  \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #reshape fc1 = C*H*W\n",
    "        self.fc1 = nn.Linear(8512, 200)\n",
    "        self.fc2 = nn.Linear(200, 5)\n",
    "\n",
    "      \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.pool1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.pool2(out)\n",
    "        \n",
    "        #reshape out to be 146624\n",
    "        (_, C, H, W) = out.data.size()\n",
    "        out = out.view( -1 , C * H * W)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "    \"\"\" \n",
    "        Please do not change the functions below. \n",
    "        They will be used to test the correctness of your implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_conv1_params(self):\n",
    "        return self.conv1.__repr__()\n",
    "    \n",
    "    def get_pool1_params(self):\n",
    "        return self.pool1.__repr__()\n",
    "\n",
    "    def get_conv2_params(self):\n",
    "        return self.conv2.__repr__()\n",
    "      \n",
    "    def get_pool2_params(self):\n",
    "        return self.pool2.__repr__()\n",
    "      \n",
    "    def get_fc1_params(self):\n",
    "        return self.fc1.__repr__()\n",
    "    \n",
    "    def get_fc2_params(self):\n",
    "        return self.fc2.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Runs experiment on the model neural network given a train loader, loss function and optimizer and find validation accuracy for each epoch given the validation_loader.\n",
    "\n",
    "    Args:\n",
    "        neural_network (NN model that extends torch.nn.Module): For example, it should take an instance of either\n",
    "                                                                FeedForwardNN or ConvolutionalNN,\n",
    "        train_loader (DataLoader),\n",
    "        validation_loader (DataLoader),\n",
    "        loss_function (torch.nn.CrossEntropyLoss),\n",
    "        optimizer (optim.SGD)\n",
    "        num_epochs (number of iterations)\n",
    "    Returns:\n",
    "        tuple: First position, training accuracies of each epoch formatted in an array of shape (num_epochs,1).\n",
    "               Second position, training loss of each epoch formatted in an array of shape (num_epochs,1).\n",
    "               third position, validation accuracy of each epoch formatted in an array of shape (num_epochs,1).\n",
    "               \n",
    "\"\"\"\n",
    "\n",
    "def train_val_NN(neural_network, train_loader, validation_loader, loss_function, optimizer, num_epochs):\n",
    "    accuracy = np.empty((num_epochs,1))\n",
    "    loss_np = np.empty((num_epochs,1))\n",
    "    val_accuracy = np.empty((num_epochs,1))\n",
    "    \n",
    "    model = neural_network\n",
    "    \n",
    "    #train first \n",
    "    for epoch in range(num_epochs):\n",
    "        #train on batch\n",
    "        total_loss = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #add loss\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        #get validation accuracy for this epoch\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        for images, labels in validation_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()       \n",
    "        \n",
    "         #get training accuracy for this epoch\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy[epoch] = train_correct/train_total\n",
    "        loss_np[epoch] = total_loss\n",
    "        val_accuracy[epoch] = val_correct/val_total\n",
    "        \n",
    "    return (accuracy,loss_np,val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Runs experiment on the model neural network given a test loader, loss function and optimizer.\n",
    "\n",
    "    Args:\n",
    "        neural_network (NN model that extends torch.nn.Module): For example, it should take an instance of either\n",
    "                                                                FeedForwardNN or ConvolutionalNN,\n",
    "        test_loader (DataLoader), (make sure the loader is not shuffled)\n",
    "        loss_function (torch.nn.CrossEntropyLoss),\n",
    "        optimizer (your choice)\n",
    "        num_epochs (number of iterations)\n",
    "    Returns:\n",
    "        your predictions         \n",
    "\"\"\"\n",
    "def test_NN(neural_network, test_loader, loss_function):\n",
    "    model = neural_network\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        Preds = []\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            Preds.append(predicted)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return Preds\n",
    "# \n",
    "# \n",
    "# with open('HW4_preds.txt', 'w') as f:\n",
    "#     for item in Preds:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: double, float, float16, int64, int32, and uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-303fd3f550b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#load into dataloader/dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-730c24bc07d8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: double, float, float16, int64, int32, and uint8."
     ]
    }
   ],
   "source": [
    "# Run Baseline CNN\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#initialize params\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "neural_network = ConvolutionalNN()\n",
    "model = neural_network.to(device)\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(neural_network.parameters(), lr=learning_rate)\n",
    "num_epochs = 40\n",
    "\n",
    "\n",
    "new_x_train, new_y_train, x_val, y_val = create_validation(tweets, users)\n",
    "\n",
    "#load into dataloader/dataset\n",
    "train_dataset =  Dataset(new_x_train, new_y_train)\n",
    "validation_dataset = Dataset(x_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "accuracy,loss_np,val_accuracy = train_val_NN(neural_network, train_loader, validation_loader, loss_function, optimizer,num_epochs)\n",
    "print(accuracy)\n",
    "print(loss_np)\n",
    "print(val_accuracy)\n",
    "\n",
    "#plot per epoch\n",
    "plt.figure()\n",
    "plt.plot(accuracy, label='accuracy')\n",
    "plt.show()\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(loss_np, label='loss_np')\n",
    "plt.show()\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(val_accuracy, label='val_accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
