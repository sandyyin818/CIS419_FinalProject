{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from nltk import TweetTokenizer\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "import math\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    "import string\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "tweet_path = \"./data/tweetsUsers.csv\"\n",
    "\n",
    "#load data with pandas and return dataframes (converted to numpy arrays)\n",
    "def load_tweets(tweet_path):\n",
    "    tweets = pd.read_csv(tweet_path)\n",
    "    \n",
    "    #choose first 100 for each user for testing purposes\n",
    "    tweets = tweets.sort_values('screen_name').groupby('screen_name').head(100)\n",
    "    comments = tweets['text']\n",
    "    tweets['screen_name'][tweets['screen_name'] == 'HillaryClinton'] = 0\n",
    "    tweets['screen_name'][tweets['screen_name'] == 'AnnCoulter'] = 1\n",
    "    tweets['screen_name'][tweets['screen_name'] == 'TrumpsGAGirl'] = 2\n",
    "    tweets['screen_name'][tweets['screen_name'] == 'realDonaldTrump'] = 3\n",
    "    tweets['screen_name'][tweets['screen_name'] == 'TomiLahren'] = 4\n",
    "    tweets['screen_name'][tweets['screen_name'] == 'MADENAMERUCA'] = 5\n",
    "    tweets['screen_name'][tweets['screen_name'] == 'LastStand2019'] = 6\n",
    "    tweets['screen_name'][tweets['screen_name'] == 'Mecdty'] = 7\n",
    "    tweets['screen_name'][tweets['screen_name'] == 'Birdle_2963'] = 8\n",
    "\n",
    "#     print(tweets['screen_name'].unique())\n",
    "    users = tweets['screen_name']\n",
    "#     data.Gender[data.Gender == 'female'] = 2\n",
    "    return comments.to_numpy(), users.to_numpy()\n",
    "\n",
    "tweets, users = load_tweets(tweet_path)\n",
    "tweets = tweets[0:500] #use first 500 for testing\n",
    "users = users[0:500].astype(np.long)\n",
    "\n",
    "#remove old style retweets, hashtags and hyperlinks in a string tweet\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet) \n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "#remove punctuation and stopwords in a list of tokens\n",
    "def clean_tweets(tweet_tokens):\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if ((word not in stopwords_english) and (word not in string.punctuation)): # remove punctuation + stopwords\n",
    "#             word = stemmer.stem(word) #stem\n",
    "            tweets_clean.append(word)\n",
    "    return tweets_clean\n",
    "\n",
    "#tokenize and clean tweets: return list of dicts representing the tokens {word: True}\n",
    "def tokenize(tweets):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True)\n",
    "    count = 0\n",
    "    tokens = [tokenizer.tokenize(clean_tweet(tweet)) for tweet in tweets]\n",
    "    tokens = [clean_tweets(token) for token in tokens]\n",
    "    tokens = [\" \".join(token) for token in tokens] #convert to list of cleaned strings\n",
    "    \n",
    "    #use bag of words model\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(tokens)\n",
    "    #CHECK STEP\n",
    "#     print(X[0])\n",
    "#     print(vectorizer.get_feature_names()[3246])\n",
    "#     print(tweets[0])\n",
    "#     print(users[0])\n",
    "    X = X.toarray()\n",
    "    X = X.astype(np.float)\n",
    "    return X, len(vectorizer.get_feature_names())\n",
    "\n",
    "tokens, num_feats = tokenize(tweets) #tokens = [[0, 1, 0, 1], [0, 1, 0, 1,], ....]\n",
    "# print(tokens[0])\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################################################\n",
    "# # Extracting and loading data\n",
    "# ############################################################\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.len = len(X)           \n",
    "        if torch.cuda.is_available():\n",
    "            self.x_data = torch.from_numpy(X).float().cuda()\n",
    "            self.y_data = torch.from_numpy(y).long().cuda()\n",
    "        else:\n",
    "            self.x_data = torch.from_numpy(X).float()\n",
    "            self.y_data = torch.from_numpy(y).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Randomly choose 20 percent of the training data as validation data.\n",
    "\n",
    "    Args:\n",
    "        x_train: tweets in form list of lists []\n",
    "        y_train: training labels in list of strings (users) [realDonald, Hillary, ...]\n",
    "    Returns:\n",
    "        new_x_train: 80%\n",
    "        new_y_train\n",
    "        x_val: 20%\n",
    "        y_val\n",
    "\"\"\"\n",
    "\n",
    "def create_validation(x_train, y_train):\n",
    "    num_images = len(x_train)\n",
    "    num_validation = math.floor(.2 * num_images)    \n",
    "#     print(num_images)\n",
    "    \n",
    "    #indices of the validation set, random\n",
    "    validation_indices = random.sample(range(0, num_images), num_validation)\n",
    "    \n",
    "    #get the indices of the training set \n",
    "    training_indices = []\n",
    "    for i in range(0, num_images):\n",
    "        if i not in validation_indices:\n",
    "            training_indices.append(i)\n",
    "    \n",
    "    x_val = np.take(x_train, validation_indices, axis = 0)\n",
    "    y_val = np.take(y_train, validation_indices, axis = 0)\n",
    "    \n",
    "    new_x_train = np.take(x_train, training_indices, axis = 0)\n",
    "    new_y_train = np.take(y_train, training_indices, axis = 0)\n",
    "        \n",
    "    return new_x_train,new_y_train,x_val,y_val\n",
    "\n",
    "new_x_train, new_y_train, x_val, y_val = create_validation(tokens, users)\n",
    "# print(new_x_train)\n",
    "# print(x_val[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Feed Forward Neural Network\n",
    "############################################################\n",
    "class FeedForwardNN(nn.Module):\n",
    "    \"\"\" \n",
    "        (1) Use self.fc1 as the variable name for your first fully connected layer\n",
    "        (2) Use self.fc2 as the variable name for your second fully connected layer\n",
    "    \"\"\"\n",
    "    hidden_size = 1000\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        #first fully connected layer\n",
    "        self.fc1 = nn.Linear(input_size, self.hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(self.hidden_size, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Runs experiment on the model neural network given a train loader, loss function and optimizer and find validation accuracy for each epoch given the validation_loader.\n",
    "\n",
    "    Args:\n",
    "        neural_network (NN model that extends torch.nn.Module): For example, it should take an instance of either\n",
    "                                                                FeedForwardNN or ConvolutionalNN,\n",
    "        train_loader (DataLoader),\n",
    "        validation_loader (DataLoader),\n",
    "        loss_function (torch.nn.CrossEntropyLoss),\n",
    "        optimizer (optim.SGD)\n",
    "        num_epochs (number of iterations)\n",
    "    Returns:\n",
    "        tuple: First position, training accuracies of each epoch formatted in an array of shape (num_epochs,1).\n",
    "               Second position, training loss of each epoch formatted in an array of shape (num_epochs,1).\n",
    "               third position, validation accuracy of each epoch formatted in an array of shape (num_epochs,1).\n",
    "               \n",
    "\"\"\"\n",
    "\n",
    "def train_val_NN(neural_network, train_loader, validation_loader, loss_function, optimizer, num_epochs):\n",
    "    accuracy = np.empty((num_epochs,1))\n",
    "    loss_np = np.empty((num_epochs,1))\n",
    "    val_accuracy = np.empty((num_epochs,1))\n",
    "    \n",
    "    model = neural_network\n",
    "    \n",
    "    #train first \n",
    "    for epoch in range(num_epochs):\n",
    "        #train on batch\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #add loss\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        #get validation accuracy for this epoch\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        for images, labels in validation_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()       \n",
    "        \n",
    "         #get training accuracy for this epoch\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy[epoch] = train_correct/train_total\n",
    "        loss_np[epoch] = total_loss\n",
    "        val_accuracy[epoch] = val_correct/val_total\n",
    "        \n",
    "    return (accuracy,loss_np,val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run FeedForward\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#initialize params\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = neural_network.to(device)\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 40\n",
    "\n",
    "#load data\n",
    "y_train = users\n",
    "x_train = tweets\n",
    "\n",
    "tokenized, size = tokenize(x_train)\n",
    "# print(\"size\" + str(size))\n",
    "\n",
    "new_x_train, new_y_train, x_val, y_val = create_validation(tokenized, y_train)\n",
    "neural_network = FeedForwardNN(size, 9)\n",
    "\n",
    "#load into dataloader/dataset\n",
    "train_dataset =  Dataset(new_x_train, new_y_train)\n",
    "validation_dataset = Dataset(x_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=False)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=12, shuffle=False)\n",
    "\n",
    "ffaccuracy,ffloss_np,ffval_accuracy = train_val_NN(neural_network, train_loader, validation_loader, loss_function, optimizer,num_epochs)\n",
    "print(ffaccuracy)\n",
    "print(ffloss_np)\n",
    "print(ffval_accuracy)\n",
    "\n",
    "#plot per epoch\n",
    "plt.figure()\n",
    "plt.ylabel('epoch')\n",
    "plt.plot(ffaccuracy, label='accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "plt.figure()\n",
    "\n",
    "plt.plot(ffloss_np, label='loss_np')\n",
    "plt.legend()\n",
    "plt.ylabel('epoch')\n",
    "plt.show()\n",
    "    \n",
    "plt.figure()\n",
    "plt.legend()\n",
    "plt.plot(ffval_accuracy, label='val_accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Runs experiment on the model neural network given a test loader, loss function and optimizer.\n",
    "\n",
    "    Args:\n",
    "        neural_network (NN model that extends torch.nn.Module): For example, it should take an instance of either\n",
    "                                                                FeedForwardNN or ConvolutionalNN,\n",
    "        test_loader (DataLoader), (make sure the loader is not shuffled)\n",
    "        loss_function (torch.nn.CrossEntropyLoss),\n",
    "        optimizer (your choice)\n",
    "        num_epochs (number of iterations)\n",
    "    Returns:\n",
    "        your predictions         \n",
    "\"\"\"\n",
    "def test_NN(neural_network, test_loader, loss_function, size_test):\n",
    "    model = neural_network\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        Preds = []\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images, size_test)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            Preds.append(predicted)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return Preds\n",
    "# \n",
    "# \n",
    "# with open('HW4_preds.txt', 'w') as f:\n",
    "#     for item in Preds:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################################################\n",
    "# # Convolutional Neural Network\n",
    "# ############################################################\n",
    "# class ConvolutionalNN(nn.Module):\n",
    "#     \"\"\" \n",
    "#     (1) Use self.conv1 as the variable name for your first convolutional layer\n",
    "#         (2) Use self.pool1 as the variable name for your first pooling layer\n",
    "#         (3) Use self.conv2 as the variable name for your second convolutional layer\n",
    "#         (4) Use self.pool2 as the variable name for you second pooling layer  \n",
    "#         (5) Use self.fc1 as the variable name for your first fully connected layer\n",
    "#         (6) Use self.fc2 as the variable name for your second fully connected layer\n",
    "#     \"\"\"\n",
    "#      # Hyper-parameters \n",
    "#     input_size = 3\n",
    "#     hidden_size = 2000\n",
    "#     num_classes = 7\n",
    "#     num_epochs = 40\n",
    "#     batch_size = 64\n",
    "#     learning_rate = 0.01\n",
    "    \n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(ConvolutionalNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(2, 16, kernel_size=3, stride=1, padding=0)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "#         #reshape fc1 = C*H*W\n",
    "#         self.fc1 = nn.Linear(8512, 200)\n",
    "      \n",
    "#     def forward(self, x):\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.pool1(out)\n",
    "        \n",
    "#         #reshape out to be 146624\n",
    "#         (_, C, H, W) = out.data.size()\n",
    "#         print(\"size: \" + str(C*H*W))\n",
    "#         out = out.view( -1 , C * H * W)\n",
    "#         out = self.fc1(out)\n",
    "\n",
    "#         return out\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [16, 2, 3, 3], but got 2-dimensional input of size [64, 2805] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-0c001ad451a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mvalidation_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_val_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0ac3f5b75deb>\u001b[0m in \u001b[0;36mtrain_val_NN\u001b[0;34m(neural_network, train_loader, validation_loader, loss_function, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-f3975ec5b335>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [16, 2, 3, 3], but got 2-dimensional input of size [64, 2805] instead"
     ]
    }
   ],
   "source": [
    "# # Run Baseline CNN\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# #initialize params\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# neural_network = ConvolutionalNN()\n",
    "# model = neural_network.to(device)\n",
    "# learning_rate = 0.001\n",
    "# batch_size = 64\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adagrad(neural_network.parameters(), lr=learning_rate)\n",
    "# num_epochs = 40\n",
    "\n",
    "# new_x_train, new_y_train, x_val, y_val = create_validation(tweets, users)\n",
    "\n",
    "# new_x_train = tokenize(new_x_train)\n",
    "# x_val = tokenize(x_val)\n",
    "\n",
    "# #load into dataloader/dataset\n",
    "# train_dataset =  Dataset(new_x_train, new_y_train)\n",
    "# validation_dataset = Dataset(x_val, y_val)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "# validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# accuracy,loss_np,val_accuracy = train_val_NN(neural_network, train_loader, validation_loader, loss_function, optimizer,num_epochs)\n",
    "# print(accuracy)\n",
    "# print(loss_np)\n",
    "# print(val_accuracy)\n",
    "\n",
    "# #plot per epoch\n",
    "# plt.figure()\n",
    "# plt.plot(accuracy, label='accuracy')\n",
    "# plt.show()\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(loss_np, label='loss_np')\n",
    "# plt.show()\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(val_accuracy, label='val_accuracy')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
